# ğŸ”¥ Spark History Server MCP

[![CI](https://github.com/DeepDiagnostix-AI/spark-history-server-mcp/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/DeepDiagnostix-AI/spark-history-server-mcp/actions)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

> **ğŸ¤– Connect AI agents to Apache Spark History Server for intelligent job analysis and performance monitoring**

Transform your Spark infrastructure monitoring with AI! This Model Context Protocol (MCP) server enables AI agents to analyze job performance, identify bottlenecks, and provide intelligent insights from your Spark History Server data.

## ğŸ¯ What is This?

**Spark History Server MCP** bridges AI agents with your existing Apache Spark infrastructure, enabling:

- ğŸ” **Query job details** through natural language
- ğŸ“Š **Analyze performance metrics** across applications
- ğŸ”„ **Compare multiple jobs** to identify regressions
- ğŸš¨ **Investigate failures** with detailed error analysis
- ğŸ“ˆ **Generate insights** from historical execution data

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ¤– AI Agent/LLM] --> B[ğŸ“¡ MCP Client]
    B --> C[âš¡ Spark History MCP Server]
    C --> D[ğŸ”¥ Your Spark History Server]
    D --> E[ğŸ“„ Spark Event Logs]

    F[ğŸ”§ LangChain Agent] --> B
    G[ğŸ“± Custom AI App] --> B
    H[ğŸ”¬ MCP Inspector] --> B
```

**ğŸ”— Components:**
- **ğŸ”¥ Spark History Server**: Your existing infrastructure serving Spark event data
- **âš¡ MCP Server**: This project - provides MCP tools for querying Spark data
- **ğŸ¤– AI Agents**: LangChain, custom agents, or any MCP-compatible client

## âš¡ Quick Start

### ğŸ“‹ Prerequisites
- ğŸ”¥ Existing Spark History Server (running and accessible)
- ğŸ Python 3.12+
- âš¡ [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager

### ğŸš€ Setup & Testing
```bash
git clone https://github.com/DeepDiagnostix-AI/spark-history-server-mcp.git
cd spark-history-server-mcp

# Install Task (if not already installed)
brew install go-task  # macOS, see https://taskfile.dev/installation/ for others

# Setup and start testing
task install                    # Install dependencies
task start-spark-bg            # Start Spark History Server with sample data (default Spark 3.5.5)
# Or specify a different Spark version:
# task start-spark-bg spark_version=3.5.2
task start-mcp-bg             # Start MCP Server

# Optional: Opens MCP Inspector on http://localhost:6274 for interactive testing
# Requires Node.js: 22.7.5+ (Check https://github.com/modelcontextprotocol/inspector for latest requirements)
task start-inspector-bg       # Start MCP Inspector

# When done, run `task stop-all`
```

### ğŸ“Š Sample Data
The repository includes real Spark event logs for testing:
- `spark-bcec39f6201b42b9925124595baad260` - âœ… Successful ETL job
- `spark-110be3a8424d4a2789cb88134418217b` - ğŸ”„ Data processing job
- `spark-cc4d115f011443d787f03a71a476a745` - ğŸ“ˆ Multi-stage analytics job

See **[TESTING.md](TESTING.md)** for using them.

### âš™ï¸ Server Configuration
Edit `config.yaml` for your Spark History Server:
```yaml
servers:
  local:
    default: true
    url: "http://your-spark-history-server:18080"
    auth:  # optional
      username: "user"
      password: "pass"
mcp:
  transports:
    - streamable-http # streamable-http or stdio.
  port: "18888"
  debug: true
```

## ğŸ“¸ Screenshots

### ğŸ” Get Spark Application
![Get Application](screenshots/get-application.png)

### âš¡ Job Performance Comparison
![Job Comparison](screenshots/job-compare.png)


## ğŸ› ï¸ Available Tools

<summary><strong>âš ï¸ Disclaimer</strong></summary>
These tools are subject to change as we scale and improve the performance of the MCP server.

### Core Analysis Tools (All Integrations)
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_application` | ğŸ“Š Get detailed application information |
| `get_jobs` | ğŸ”— List jobs within an application |
| `compare_job_performance` | ğŸ“ˆ Compare performance between applications |
| `compare_sql_execution_plans` | ğŸ” Compare SQL execution plans |
| `get_job_bottlenecks` | ğŸš¨ Identify performance bottlenecks |
| `get_slowest_jobs` | â±ï¸ Find slowest jobs in application |

### Additional Tools (LlamaIndex/LangGraph HTTP Mode)
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `list_applications` | ğŸ“‹ List Spark applications with filtering |
| `get_application_details` | ğŸ“Š Get comprehensive application info |
| `get_stage_details` | âš¡ Analyze stage-level metrics |
| `get_task_details` | ğŸ¯ Examine individual task performance |
| `get_executor_summary` | ğŸ–¥ï¸ Review executor utilization |
| `get_application_environment` | âš™ï¸ Review Spark configuration |
| `get_storage_info` | ğŸ’¾ Analyze RDD storage usage |
| `get_sql_execution_details` | ğŸ” Deep dive into SQL queries |
| `get_resource_usage_timeline` | ğŸ“ˆ Resource allocation over time |
| `compare_job_environments` | âš™ï¸ Compare Spark configurations |
| `get_slowest_stages` | â±ï¸ Find slowest stages |
| `get_task_metrics` | ğŸ“Š Detailed task performance metrics |

## ğŸš€ Production Deployment

Deploy using Kubernetes with Helm:

> âš ï¸ **Work in Progress**: We are still testing and will soon publish the container image and Helm registry to GitHub for easy deployment.

```bash
# ğŸ“¦ Deploy with Helm
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/

# ğŸ¯ Production configuration
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/ \
  --set replicaCount=3 \
  --set autoscaling.enabled=true \
  --set monitoring.enabled=true
```

ğŸ“š See [`deploy/kubernetes/helm/`](deploy/kubernetes/helm/) for complete deployment manifests and configuration options.

## ğŸŒ Multi-Spark History Server Setup
Setup multiple Spark history servers in the config.yaml and choose which server you want the LLM to interact with for each query.

```yaml
servers:
  production:
    default: true
    url: "http://prod-spark-history:18080"
    auth:
      username: "user"
      password: "pass"
  staging:
    url: "http://staging-spark-history:18080"
```

ğŸ’ User Query: "Can you get application <app_id> using production server?"

ğŸ¤– AI Tool Request:
```
{
  "spark_id": "<app_id>",
  "server": "production"
}
```
ğŸ¤– AI Tool Response:
```
{
  "id": "<app_id>>",
  "name": "app_name",
  "coresGranted": null,
  "maxCores": null,
  "coresPerExecutor": null,
  "memoryPerExecutorMB": null,
  "attempts": [
    {
      "attemptId": null,
      "startTime": "2023-09-06T04:44:37.006000Z",
      "endTime": "2023-09-06T04:45:40.431000Z",
      "lastUpdated": "2023-09-06T04:45:42Z",
      "duration": 63425,
      "sparkUser": "spark",
      "appSparkVersion": "3.3.0",
      "completed": true
    }
  ]
}
```

### ğŸ” Environment Variables
```bash
SHS_SPARK_USERNAME=your_username
SHS_SPARK_PASSWORD=your_password
SHS_SPARK_TOKEN=your_jwt_token
SHS_MCP_PORT=18888
SHS_MCP_DEBUG=false
SHS_MCP_ADDRESS=0.0.0.0
```

## ğŸ¤– AI Agent Integration

### Quick Start Options

| Integration | Transport | Best For |
|-------------|-----------|----------|
| **[Local Testing](TESTING.md)** | HTTP | Development, testing tools |
| **[Claude Desktop](examples/integrations/claude-desktop/)** | STDIO | Interactive analysis |
| **[Amazon Q CLI](examples/integrations/amazon-q-cli/)** | STDIO | Command-line automation |
| **[LlamaIndex](examples/integrations/llamaindex.md)** | HTTP | Knowledge systems, RAG |
| **[LangGraph](examples/integrations/langgraph.md)** | HTTP | Multi-agent workflows |

## ğŸ¯ Example Use Cases

### ğŸ” Performance Investigation
```
ğŸ¤– AI Query: "Why is my ETL job running slower than usual?"

ğŸ“Š MCP Actions:
âœ… Analyze application metrics
âœ… Compare with historical performance
âœ… Identify bottleneck stages
âœ… Generate optimization recommendations
```

### ğŸš¨ Failure Analysis
```
ğŸ¤– AI Query: "What caused job 42 to fail?"

ğŸ” MCP Actions:
âœ… Examine failed tasks and error messages
âœ… Review executor logs and resource usage
âœ… Identify root cause and suggest fixes
```

### ğŸ“ˆ Comparative Analysis
```
ğŸ¤– AI Query: "Compare today's batch job with yesterday's run"

ğŸ“Š MCP Actions:
âœ… Compare execution times and resource usage
âœ… Identify performance deltas
âœ… Highlight configuration differences
```

## ğŸ“” Spark History Server Setup

### Setting up Native Spark History Server with MCP

Prereqs
- ğŸ³ [Docker](https://docs.docker.com/desktop/setup/install/mac-install/) must be installed and running

Steps
- Run `task start-spark`

*Optional: Specify a different Spark version using the `spark_version` parameter:*
```bash
task start-spark spark_version=3.5.2  # Use Spark 3.5.2 instead of default 3.5.5
```
*Although Spark UI is mostly backwards compatible, it is best to use the same Spark History Server version as the version of your Spark application that you are investigating.*

Setup Issues
```bash
# If you get "Docker not running" error:
./start_local_spark_history.sh --dry-run  # Check prerequisites

# If you get "No containers to stop" warning:
# This is normal - just means no previous containers are running

# To get help with script options:
./start_local_spark_history.sh --help
```

### Setting up AWS Glue Spark History Server with MCP

There are two documented methods of setting up Spark history server to Glue

#### Option 1: Launching the Spark history server and viewing the Spark UI using AWS CloudFormation

[Public Documentation](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-history.html#monitor-spark-ui-history-cfn)

Prereqs
- Ensure you follow the public documentation and setup a Spark Web UI
- Identify the SparkUiPrivateUrl or SparkUiPublicUrl and ensure you can open it in the web browser

Steps
- Open [config.yaml](config.yaml) and add the Web UI URL to your server
``` example
glue_ec2:
  url: "<SparkUiUrl>:<port>"
  verify_ssl: true
  auth:
    username: "staging_user"
    password: "your_password"
    token: "staging_token"
```

Setup Issues
```bash
# If you get "CERTIFICATE_VERIFY_FAILED" error
set verify_ssl: false
```

âš ï¸ **WARNING**: If verify_ssl is set to False, SSL certificate verification will be disabled.
  This is insecure and should only be used in development environments

#### Option 2: Launching the Spark history server and viewing the Spark UI using Docker

[Public Documentation](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-history.html#monitor-spark-ui-history-local)

Prereqs
- ğŸ³ [Docker](https://docs.docker.com/desktop/setup/install/mac-install/) must be installed and running

Steps:
- Follow steps in public doc to get Docker running http://localhost:18080 in your browser
- Ensure [config.yaml](config.yaml) local server has correct url:
```
local:
    default: true  # if server name is not provided in tool calls, this Spark History Server is used
    url: "http://localhost:18080"
    verify_ssl: true
    # Optional authentication (can also use environment variables)
    # auth:
    #   username: "your_username"  # or use SHS_SPARK_USERNAME env var
    #   password: "your_password"  # or use SHS_SPARK_PASSWORD env var
    #   token: "your_token"       # or use SHS_SPARK_TOKEN env var
```

## ğŸ¤ Contributing

Check [CONTRIBUTING.md](CONTRIBUTING.md) for full guidelines on contributions

## ğŸ“„ License

Apache License 2.0 - see [LICENSE](LICENSE) file for details.


---

<div align="center">

**ğŸ”¥ Connect your Spark infrastructure to AI agents**

[ğŸš€ Get Started](#-quick-start) | [ğŸ› ï¸ View Tools](#%EF%B8%8F-available-tools) | [ğŸ§ª Test Now](TESTING.md) | [ğŸ¤ Contribute](#-contributing)

*Built by the community, for the community* ğŸ’™

</div>
