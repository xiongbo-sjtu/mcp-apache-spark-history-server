# ğŸ”¥ Spark History Server MCP

[![CI](https://github.com/DeepDiagnostix-AI/spark-history-server-mcp/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/DeepDiagnostix-AI/spark-history-server-mcp/actions)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

> **ğŸ¤– Connect AI agents to Apache Spark History Server for intelligent job analysis and performance monitoring**

Transform your Spark infrastructure monitoring with AI! This Model Context Protocol (MCP) server enables AI agents to analyze job performance, identify bottlenecks, and provide intelligent insights from your Spark History Server data.

## ğŸ¯ What is This?

**Spark History Server MCP** bridges AI agents with your existing Apache Spark infrastructure, enabling:

- ğŸ” **Query job details** through natural language
- ğŸ“Š **Analyze performance metrics** across applications
- ğŸ”„ **Compare multiple jobs** to identify regressions
- ğŸš¨ **Investigate failures** with detailed error analysis
- ğŸ“ˆ **Generate insights** from historical execution data

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ¤– AI Agent/LLM] --> B[ğŸ“¡ MCP Client]
    B --> C[âš¡ Spark History MCP Server]
    C --> D[ğŸ”¥ Your Spark History Server]
    D --> E[ğŸ“„ Spark Event Logs]

    F[ğŸ”§ LangChain Agent] --> B
    G[ğŸ“± Custom AI App] --> B
    H[ğŸ”¬ MCP Inspector] --> B
```

**ğŸ”— Components:**
- **ğŸ”¥ Spark History Server**: Your existing infrastructure serving Spark event data
- **âš¡ MCP Server**: This project - provides MCP tools for querying Spark data
- **ğŸ¤– AI Agents**: LangChain, custom agents, or any MCP-compatible client

## âš¡ Quick Start

### ğŸ“‹ Prerequisites
- ğŸ”¥ Existing Spark History Server (running and accessible)
- ğŸ Python 3.12+
- âš¡ [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager

### ğŸš€ Setup
```bash
git clone https://github.com/DeepDiagnostix-AI/spark-history-server-mcp.git
cd spark-history-server-mcp
uv sync --frozen
uv run main.py
```

### âš™ï¸ Configuration
Edit `config.yaml`:
```yaml
servers:
  local:
    default: true # if server name is not provided in tool calls, this Spark History Server is used
    url: "http://your-spark-history-server:18080"
    auth:  # optional
      username: "user"
      password: "pass"
```

### ğŸ”¬ Testing with MCP Inspector
```bash
# Start MCP server with Inspector (opens browser automatically)
npx @modelcontextprotocol/inspector
```

**ğŸŒ Test in Browser** - The MCP Inspector opens at http://localhost:6274 for interactive tool testing!

## ğŸ“¸ Screenshots

### ğŸ” Get Spark Application
![Get Application](screenshots/get-application.png)

### âš¡ Job Performance Comparison
![Job Comparison](screenshots/job-compare.png)

## ğŸ› ï¸ Available Tools

### ğŸ“Š Application & Job Analysis
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_application` | Get detailed information about a specific Spark application |
| `get_jobs` | Get a list of all jobs for a Spark application |
| `get_slowest_jobs` | Get the N slowest jobs for a Spark application |

### âš¡ Stage & Task Analysis
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_stages` | Get a list of all stages for a Spark application |
| `get_slowest_stages` | Get the N slowest stages for a Spark application |
| `get_stage` | Get information about a specific stage |
| `get_stage_task_summary` | Get task metrics summary for a specific stage |

### ğŸ–¥ï¸ Executor & Resource Analysis
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_executors` | Get executor information for an application |
| `get_executor` | Get information about a specific executor |
| `get_executor_summary` | Get aggregated metrics across all executors |
| `get_resource_usage_timeline` | Get resource usage timeline for an application |

### ğŸ” SQL & Performance Analysis
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_slowest_sql_queries` | Get the top N slowest SQL queries for an application |
| `get_job_bottlenecks` | Identify performance bottlenecks in a Spark job |
| `get_environment` | Get comprehensive Spark runtime configuration |

### ğŸ“ˆ Comparison Tools
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `compare_job_performance` | Compare performance metrics between two Spark jobs |
| `compare_job_environments` | Compare Spark environment configurations between two jobs |
| `compare_sql_execution_plans` | Compare SQL execution plans between two Spark jobs |

## ğŸš€ Production Deployment

Deploy using Kubernetes with Helm:

> âš ï¸ **Work in Progress**: We are still testing and will soon publish the container image and Helm registry to GitHub for easy deployment.

```bash
# ğŸ“¦ Deploy with Helm
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/

# ğŸ¯ Production configuration
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/ \
  --set replicaCount=3 \
  --set autoscaling.enabled=true \
  --set monitoring.enabled=true
```

ğŸ“š See [`deploy/kubernetes/helm/`](deploy/kubernetes/helm/) for complete deployment manifests and configuration options.

## ğŸ§ª Testing & Development

### ğŸ”¬ Local Development

#### ğŸ“‹ Prerequisites
- Install [Task](https://taskfile.dev/installation/) for running development commands:
  ```bash
  # macOS
  brew install go-task

  # Other platforms - see https://taskfile.dev/installation/
  ```

*Note: uv will be automatically installed when you run `task install`*

#### ğŸš€ Development Commands

**Quick Setup:**
```bash
# ğŸ“¦ Install dependencies and setup pre-commit hooks
task install
task pre-commit-install

# ğŸš€ Start services one by one (all in background)
task start-spark-bg      # Start Spark History Server
task start-mcp-bg        # Start MCP Server
task start-inspector-bg  # Start MCP Inspector

# ğŸŒ Then open http://localhost:6274 in your browser

# ğŸ›‘ When done, stop all services
task stop-all
```

**Essential Commands:**
```bash

# ğŸ›‘ Stop all background services
task stop-all

# ğŸ§ª Run tests and checks
task test               # Run pytest
task lint               # Check code style
task pre-commit         # Run all pre-commit hooks
task validate           # Run lint + tests

# ğŸ”§ Development utilities
task format             # Auto-format code
task clean              # Clean build artifacts
```

*For complete command reference, see `Taskfile.yml`*

### ğŸ“Š Sample Data
The repository includes real Spark event logs for testing:
- `spark-bcec39f6201b42b9925124595baad260` - âœ… Successful ETL job
- `spark-110be3a8424d4a2789cb88134418217b` - ğŸ”„ Data processing job
- `spark-cc4d115f011443d787f03a71a476a745` - ğŸ“ˆ Multi-stage analytics job

They are available in the [`examples/basic/events`](examples/basic/events) directory.
The [`start_local_spark_history.sh`](start_local_spark_history.sh) script automatically makes them available for local testing.

ğŸ“– **Complete testing guide**: **[TESTING.md](TESTING.md)**

## âš™ï¸ Configuration

### ğŸŒ Multi-server Setup
```yaml
servers:
  production:
    default: true
    url: "http://prod-spark-history:18080"
    auth:
      username: "user"
      password: "pass"
  staging:
    url: "http://staging-spark-history:18080"
```

### ğŸ” Environment Variables
```bash
SPARK_USERNAME=your_username
SPARK_PASSWORD=your_password
SPARK_TOKEN=your_jwt_token
MCP_PORT=18888
MCP_DEBUG=false
```

## ğŸ¤– AI Agent Integration

For production AI agent integration, see [`examples/integrations/`](examples/integrations/):

- ğŸ¦™ [LlamaIndex](examples/integrations/llamaindex.md) - Vector indexing and search
- ğŸ”— [LangGraph](examples/integrations/langgraph.md) - Multi-agent workflows

ğŸ§ª **For local testing and development, use [TESTING.md](TESTING.md) with MCP Inspector.**

## ğŸ¯ Example Use Cases

### ğŸ” Performance Investigation
```
ğŸ¤– AI Query: "Why is my ETL job running slower than usual?"

ğŸ“Š MCP Actions:
âœ… Analyze application metrics
âœ… Compare with historical performance
âœ… Identify bottleneck stages
âœ… Generate optimization recommendations
```

### ğŸš¨ Failure Analysis
```
ğŸ¤– AI Query: "What caused job 42 to fail?"

ğŸ” MCP Actions:
âœ… Examine failed tasks and error messages
âœ… Review executor logs and resource usage
âœ… Identify root cause and suggest fixes
```

### ğŸ“ˆ Comparative Analysis
```
ğŸ¤– AI Query: "Compare today's batch job with yesterday's run"

ğŸ“Š MCP Actions:
âœ… Compare execution times and resource usage
âœ… Identify performance deltas
âœ… Highlight configuration differences
```

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create feature branch: `git checkout -b feature/new-tool`
3. ğŸ§ª Add tests for new functionality
4. âœ… Run tests: `task test`
5. ğŸ“¤ Submit pull request

## ğŸ“„ License

Apache License 2.0 - see [LICENSE](LICENSE) file for details.


---

<div align="center">

**ğŸ”¥ Connect your Spark infrastructure to AI agents**

[ğŸš€ Get Started](#-quick-start) | [ğŸ› ï¸ View Tools](#%EF%B8%8F-available-tools) | [ğŸ§ª Test Now](TESTING.md) | [ğŸ¤ Contribute](#-contributing)

*Built by the community, for the community* ğŸ’™

</div>
